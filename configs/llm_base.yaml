seed: 2025

tokenizer:
  # 'char' | 'bpe_simple' | 'hf'
  mode: "char"
  vocab_size: 8000            # 对 hf 可忽略；对 bpe_simple 控制上限
  pretrained_name: "gpt2"     # 仅在 mode=='hf' 时使用

data:
  # 支持通配：/kaggle/input/your-dataset/**/*.txt
  text_glob: "./data/**/*.txt"
  train_val_split: 0.98

model:
  block_size: 256
  n_layer: 6
  n_head: 8
  n_embd: 512
  dropout: 0.1
  tie_weights: true

train:
  epochs: 1
  batch_size: 16
  lr: 3.0e-4
  weight_decay: 0.1
  warmup_steps: 1000
  grad_accum_steps: 4
  max_steps: 1000           # 限制步数，方便调试；设为空或删除则按 epochs
  amp: true
  compile: false
  out_dir: "/kaggle/working/outputs"
  min_lr: 0.0
